{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json \n",
    "from utils import detokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_time_file = \"/brtx/602-nvme1/estengel/ambiguous_parsing/logs/1.0/codegen-16B_lamp_no_context_all_scope_fol_0_test_eval_constrained_bs_5_np_full/model_outputs.20230418T115331.jsonl\"\n",
    "forced_file = \"/brtx/602-nvme1/estengel/ambiguous_parsing/model_outputs/codegen-16B/scope_fol/outputs/test_eval.logits\"\n",
    "gold_file_test = \"/brtx/602-nvme1/estengel/ambiguous_parsing/data/processed/scope_fol/test.jsonl\"\n",
    "gold_file_eval = \"/brtx/602-nvme1/estengel/ambiguous_parsing/data/processed/scope_fol/test_eval.jsonl\"\n",
    "\n",
    "\n",
    "with open(gold_file_test) as f1, open(gold_file_eval) as f2:\n",
    "    gold_test = [json.loads(line) for line in f1]\n",
    "    gold_eval = [json.loads(line) for line in f2]\n",
    "    gold_eval_by_src = {d['utterance']:[] for d in gold_eval}\n",
    "    for i, d in enumerate(gold_eval):\n",
    "        gold_eval_by_src[d['utterance']].append((i,d))\n",
    "\n",
    "\n",
    "with open(test_time_file) as f1, open(forced_file) as f2:\n",
    "    test_time = [json.loads(line) for line in f1]\n",
    "    forced = [json.loads(line) for line in f2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 0\n",
    "\n",
    "test_line = test_time[test_idx]\n",
    "\n",
    "eval_idxs, gold_eval_lines = zip(*gold_eval_by_src[test_line['test_datum_natural']])\n",
    "\n",
    "forced_line_0 = forced[eval_idxs[0]]\n",
    "forced_line_1 = forced[eval_idxs[1]]\n",
    "\n",
    "assert(test_line['test_datum_natural'] == forced_line_0['natural'])\n",
    "assert(test_line['test_datum_natural'] == forced_line_1['natural'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold program 0:\n",
      "\texists x . forall y . exists a . boy(y) AND dog(x) AND observed(a) AND agent(a, y) AND patient(a, x)\n",
      "gold program 1:\n",
      "\tforall x . exists y . exists a . boy(x) AND dog(y) AND observed(a) AND agent(a, x) AND patient(a, y)\n",
      "test-time predicted program:\n",
      "\tforall x . exists y . exists a . agent(a, x) AND boy(x) AND dog(y) AND observed(a) AND patient(a, y)\n"
     ]
    }
   ],
   "source": [
    "gold_eval_line_0 = [x for x in gold_eval_lines if str(x['template_idx']) == \"0\"][0]\n",
    "gold_eval_line_1 = [x for x in gold_eval_lines if str(x['template_idx']) == \"1\"][0]\n",
    "\n",
    "print(f\"gold program 0:\\n\\t{gold_eval_line_0['plan']}\")\n",
    "print(f\"gold program 1:\\n\\t{gold_eval_line_1['plan']}\")\n",
    "\n",
    "print(f\"test-time predicted program:\\n\\t{test_line['outputs'][0]}\")\n",
    "# print(f\"forced program 0:\\n\\t{forced_line_0['labels']}\")\n",
    "# print(f\"forced program 1:\\n\\t{forced_line_1['labels']}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re \n",
    "\n",
    "def detokenize(tokenizer,\n",
    "               delimiter: str, \n",
    "               top_logits: np.array, \n",
    "               tokens: np.array,\n",
    "               agg_fxn=np.min):\n",
    "            \n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokens)\n",
    "    # convert to detokenized \n",
    "    tok_idx_to_str_idx = {}\n",
    "    str_idx_to_tok_idx = defaultdict(list)\n",
    "    str_toks = []\n",
    "    curr_tok = []\n",
    "    str_idx = -1\n",
    "\n",
    "    # add last token\n",
    "    str_toks.append(curr_tok)\n",
    "    for i, tok in enumerate(tokens[0:-1]):\n",
    "        if tok is None:\n",
    "            print(\"None token\")\n",
    "            continue\n",
    "        # is not a subword \n",
    "        if tok.startswith(delimiter):\n",
    "            if len(curr_tok) > 0:\n",
    "                str_toks.append(curr_tok)\n",
    "            # start of a new token  \n",
    "            curr_tok = [tok]\n",
    "            str_idx += 1\n",
    "            tok_idx_to_str_idx[i] = str_idx\n",
    "            str_idx_to_tok_idx[str_idx].append(i)\n",
    "\n",
    "        # is a subword \n",
    "        else:\n",
    "            # add to curr tok\n",
    "            curr_tok.append(tok)\n",
    "            tok_idx_to_str_idx[i] = str_idx\n",
    "            str_idx_to_tok_idx[str_idx].append(i) \n",
    "\n",
    "    # add last token\n",
    "    str_toks.append(curr_tok)\n",
    "    token_ids = np.ones(len(tokens)) * -1\n",
    "    for i, token in enumerate(str_toks): \n",
    "        token = \"\".join(token).upper()\n",
    "        token = re.sub(f\"^{delimiter}\", \"\", token)\n",
    "        mapping_idxs = str_idx_to_tok_idx[i]\n",
    "        # rules:\n",
    "        for idx in mapping_idxs:\n",
    "            token_ids[idx] = i\n",
    "\n",
    "    # average logits for each token \n",
    "    new_types = []\n",
    "    new_tokens = []\n",
    "    new_top_logits = []\n",
    "    prev_id = -1\n",
    "    curr_logits = []\n",
    "    for token_id, top_logit in zip(token_ids, top_logits):\n",
    "        if token_id != prev_id and len(curr_logits) > 0:\n",
    "            # new token, check old token  \n",
    "            # average logits \n",
    "            mean_logit = agg_fxn(curr_logits)\n",
    "\n",
    "            new_types.append(type)\n",
    "            new_tokens.append(prev_id)\n",
    "            new_top_logits.append(mean_logit)\n",
    "            # whole token correct iff idxs all are correct \n",
    "            # is_correct = np.all(self.check_tokens(curr_idxs, curr_labs))\n",
    "            # new_is_correct.append(is_correct)     \n",
    "            # initialize with new token\n",
    "            curr_logits = [top_logit]\n",
    "        else:\n",
    "            # add \n",
    "            curr_logits.append(top_logit)\n",
    "\n",
    "        prev_id = token_id\n",
    "\n",
    "    # once at the end       \n",
    "    # average logits \n",
    "    mean_logit = agg_fxn(curr_logits)\n",
    "    new_tokens.append(token_id)\n",
    "    new_top_logits.append(mean_logit)\n",
    "    # is_correct = True\n",
    "    # whole token correct iff idxs all are correct \n",
    "    # is_correct = np.all(self.check_tokens(curr_idxs, curr_labs))\n",
    "    # new_is_correct.append(is_correct)     \n",
    "\n",
    "    top_logits = np.array(new_top_logits)     \n",
    "    # is_correct = np.array(new_is_correct)\n",
    "    new_toks = str_toks[1:]\n",
    "    new_toks = [re.sub(delimiter, \"\", \"\".join(tok)) for tok in new_toks]\n",
    "    return top_logits, new_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tforall\t0.5648767349390964\n",
      "1\tx\t0.985724482373312\n",
      "2\t.\t0.9453116880451221\n",
      "3\texists\t0.7144664836347207\n",
      "4\ty\t0.8712118520967571\n",
      "5\t.\t0.9961478662309687\n",
      "6\texists\t0.9105299804301655\n",
      "7\ta\t0.9938868774578484\n",
      "8\t.\t0.9952960603439626\n",
      "9\tboy(x)\t0.8077777556233645\n",
      "10\tAND\t0.969387801750199\n",
      "11\tdog(y)\t0.6645326153535681\n",
      "12\tAND\t0.9944135269122064\n",
      "13\tobserved(a)\t0.987998944035489\n",
      "14\tAND\t0.9809226256682486\n",
      "15\tagent(a,\t0.9807034735989362\n",
      "16\tx)\t0.9958641628325815\n",
      "17\tAND\t0.9938530828443801\n",
      "18\tpatient(a,\t0.9990571099108826\n",
      "19\ty\t0.9991749185659584\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/brtx/601-nvme1/estengel/.cache/codegen-350M/\")\n",
    "\n",
    "logprobs = np.exp(test_line['results'][0]['logprobs'])\n",
    "tokens = test_line['results'][0]['tokens']\n",
    "\n",
    "new_logits, new_tokens = detokenize(tokenizer, \"Ä \", logprobs, tokens, agg_fxn=np.min)\n",
    "\n",
    "\n",
    "for i, (tok, logit) in enumerate(zip(new_tokens, new_logits)):\n",
    "    print(f\"{i}\\t{tok}\\t{logit}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
